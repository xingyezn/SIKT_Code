{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f48ff2c1",
   "metadata": {},
   "source": [
    "# Get Side Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cea502",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-06T12:52:54.434421Z",
     "start_time": "2022-08-06T12:52:09.018140Z"
    },
    "code_folding": [
     9,
     32,
     56,
     67,
     80,
     94
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run this code to get side information in floder 'info_data/'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import datetime\n",
    "import os\n",
    "tqdm_ncols = 80\n",
    "def updata_data(old_data, one_key, one_row_data):\n",
    "    if one_key not in old_data.keys():\n",
    "        old_data[one_key] = [\n",
    "            one_row_data['is_right'], one_row_data['use_time'], 1\n",
    "        ]\n",
    "        return old_data, [0, 0, 0]\n",
    "    else:\n",
    "        one_data = old_data[one_key]\n",
    "        re_data = [(one_data[0] / one_data[2]) * 100,\n",
    "                   one_data[1] / one_data[2], one_data[2]]\n",
    "        one_data[0] += one_row_data['is_right']\n",
    "        one_data[1] += one_row_data['use_time']\n",
    "        one_data[2] += 1\n",
    "        old_data[one_key] = one_data\n",
    "        return old_data, re_data\n",
    "\n",
    "\n",
    "def updata_answer_data(old_data, one_key, one_row_data):\n",
    "    if one_key not in old_data.keys():\n",
    "        old_data[one_key] = [\n",
    "            one_row_data['is_right'], one_row_data['use_time'], 1\n",
    "        ]\n",
    "        return old_data, [\n",
    "            one_row_data['is_right'] * 100, one_row_data['use_time'], 1\n",
    "        ]\n",
    "    else:\n",
    "        one_data = old_data[one_key]\n",
    "        one_data[0] += one_row_data['is_right']\n",
    "        one_data[1] += one_row_data['use_time']\n",
    "        one_data[2] += 1\n",
    "        old_data[one_key] = one_data\n",
    "        re_data = [(one_data[0] / one_data[2]) * 100,\n",
    "                   one_data[1] / one_data[2], one_data[2]]\n",
    "        return old_data, re_data\n",
    "\n",
    "\n",
    "def get_all_info(all_info, col_name, col_value, new_record_info):\n",
    "    if col_value not in all_info.keys():\n",
    "        one_q_data = new_record_info[new_record_info[col_name] == col_value]\n",
    "        all_info[col_value] = [\n",
    "            np.mean(one_q_data['is_right']),\n",
    "            np.mean(one_q_data['use_time']),\n",
    "            len(one_q_data.index)\n",
    "        ]\n",
    "    return all_info\n",
    "\n",
    "\n",
    "def updata_time_data(old_data, one_key, create_time_now):\n",
    "    if one_key not in old_data.keys():\n",
    "        old_data[one_key] = create_time_now\n",
    "        return old_data, 0\n",
    "    else:\n",
    "        old_time = old_data[one_key]\n",
    "        re_data = create_time_now - old_time\n",
    "        old_data[one_key] = create_time_now\n",
    "        return old_data, re_data\n",
    "    \n",
    "def updata_passed_data(old_data, one_key, now_index):\n",
    "    if one_key not in old_data.keys():\n",
    "        old_data[one_key] = [now_index]\n",
    "        return old_data, []\n",
    "    else:\n",
    "        re_data = old_data[one_key].copy()\n",
    "        old_data[one_key].append(now_index)\n",
    "        return old_data, re_data\n",
    "\n",
    "\n",
    "def get_data(raw_data, stu_kp_data, stu_q_data, kp_data, q_data, q_all_data,\n",
    "             kp_all_data, stu_kp_data_an, stu_q_data_an, kp_data_an,\n",
    "             q_data_an,last_kp_time,last_q_time,passed_kp_list,passed_q_list):\n",
    "    all_data = []\n",
    "    for i in tqdm.tqdm(raw_data.index,\n",
    "                       desc='Process',\n",
    "                       mininterval=0.2,\n",
    "                       ncols=tqdm_ncols):\n",
    "        one_data = []\n",
    "        one_re_data = raw_data.loc[i]\n",
    "        stu_id = one_re_data['stu_id']\n",
    "        kp_id = one_re_data['kp_id']\n",
    "        q_id = one_re_data['q_id']\n",
    "        stu_kp = str(int(stu_id)) + '_' + str(int(kp_id))\n",
    "        one_time = one_re_data['create_time']\n",
    "\n",
    "        stu_kp_data, one_stu_kp_data = updata_data(\n",
    "            stu_kp_data,\n",
    "            stu_kp, one_re_data)\n",
    "        one_data.extend(one_stu_kp_data)\n",
    "\n",
    "        # 'stu_q_now_acc', 'stu_q_now_time', 'stu_q_now_len'\n",
    "        stu_q_data, one_stu_q_data = updata_data(stu_q_data, str(int(stu_id)),\n",
    "                                                 one_re_data)\n",
    "        one_data.extend(one_stu_q_data)\n",
    "\n",
    "        # 'q_now_acc','q_now_use_time','q_now_len'\n",
    "        q_data, one_q_data = updata_data(q_data, str(int(q_id)), one_re_data)\n",
    "        one_data.extend(one_q_data)\n",
    "        \n",
    "        # 'kp_now_acc','kp_now_use_time','kp_now_len'\n",
    "        kp_data, one_kp_data = updata_data(kp_data, str(int(kp_id)),\n",
    "                                           one_re_data)\n",
    "        one_data.extend(one_kp_data)\n",
    "\n",
    "        # 'q_all_acc','q_all_use_time','q_all_len'\n",
    "        q_all_data = get_all_info(q_all_data, 'q_id', q_id, raw_data)\n",
    "        one_data.extend(q_all_data[q_id])\n",
    "\n",
    "        # 'kp_all_acc','kp_all_use_time','kp_all_len'\n",
    "        kp_all_data = get_all_info(kp_all_data, 'kp_id', kp_id, raw_data)\n",
    "        one_data.extend(kp_all_data[kp_id])\n",
    "\n",
    "        # 'stu_kp_now_acc_an','stu_kp_now_use_time_an','stu_kp_now_len_an'\n",
    "        stu_kp_data_an, one_stu_kp_data_an = updata_answer_data(\n",
    "            stu_kp_data_an,\n",
    "            str(int(stu_id)) + '_' + str(int(kp_id)), one_re_data)\n",
    "        one_data.extend(one_stu_kp_data_an)\n",
    "\n",
    "        # 'stu_q_now_acc_an', 'stu_q_now_time_an', 'stu_q_now_len_an'\n",
    "        stu_q_data_an, one_stu_q_data_an = updata_answer_data(\n",
    "            stu_q_data_an, str(int(stu_id)), one_re_data)\n",
    "        one_data.extend(one_stu_q_data_an)\n",
    "\n",
    "        # 'q_now_acc_an','q_now_use_time_an','q_now_len_an'\n",
    "        q_data_an, one_q_data_an = updata_answer_data(q_data_an,\n",
    "                                                      str(int(q_id)),\n",
    "                                                      one_re_data)\n",
    "        one_data.extend(one_q_data_an)\n",
    "        # 'kp_now_acc_an','kp_now_use_time_an','kp_now_len_an'\n",
    "        kp_data_an, one_kp_data_an = updata_answer_data(\n",
    "            kp_data_an, str(int(kp_id)), one_re_data)\n",
    "        one_data.extend(one_kp_data_an)\n",
    "        \n",
    "        # 'last_kp_time_passed','last_q_time_passed'\n",
    "        last_kp_time,last_kp_time_passed = updata_time_data(last_kp_time,stu_kp, one_time)\n",
    "        last_q_time,last_q_time_passed = updata_time_data(last_q_time,str(int(stu_id)),one_time)\n",
    "        one_data.extend([last_kp_time_passed,last_q_time_passed])\n",
    "        \n",
    "        # 'passed_kp','passed_q'\n",
    "        passed_kp_list,passed_kp = updata_passed_data(passed_kp_list,stu_kp,i)\n",
    "        passed_q_list,passed_q = updata_passed_data(passed_q_list,str(int(stu_id)),i)\n",
    "        one_data.extend([passed_kp,passed_q])\n",
    "        \n",
    "        all_data.append(one_data)\n",
    "    all_data_2 = np.array(all_data,dtype=object)\n",
    "    new_col = [\n",
    "        'stu_kp_now_acc', 'stu_kp_now_use_time', 'stu_kp_now_len',\n",
    "        'stu_q_now_acc', 'stu_q_now_time', 'stu_q_now_len', \n",
    "        'q_now_acc', 'q_now_use_time', 'q_now_len', \n",
    "        'kp_now_acc', 'kp_now_use_time','kp_now_len', \n",
    "        'q_all_acc', 'q_all_use_time', 'q_all_len', \n",
    "        'kp_all_acc','kp_all_use_time', 'kp_all_len', \n",
    "        'stu_kp_now_acc_an','stu_kp_now_use_time_an', 'stu_kp_now_len_an', \n",
    "        'stu_q_now_acc_an','stu_q_now_time_an', 'stu_q_now_len_an', \n",
    "        'q_now_acc_an','q_now_use_time_an', 'q_now_len_an', \n",
    "        'kp_now_acc_an','kp_now_use_time_an', 'kp_now_len_an',\n",
    "        'last_kp_time_passed','last_q_time_passed','passed_kp','passed_q'\n",
    "    ]\n",
    "    processed_data = raw_data.copy()\n",
    "    for j, one_col in enumerate(new_col):\n",
    "        processed_data[one_col] = all_data_2[:, j]\n",
    "    return processed_data\n",
    "\n",
    "# Side information extraction\n",
    "all_dataset = [ 'ASSIST2009NSB', 'ASSIST2012', 'EdNet']\n",
    "for one_dataset in all_dataset:\n",
    "    data_name = one_dataset\n",
    "    print(f'{data_name} processing')\n",
    "    raw_data = pd.read_pickle(f'raw_data/{data_name}.pkl')\n",
    "    print(raw_data.head())\n",
    "    if data_name in ['ASSIST2012']:\n",
    "        raw_data['create_time'] = pd.to_datetime(raw_data['create_time'])\n",
    "        raw_data['create_time'] =pd.to_timedelta(raw_data['create_time']-raw_data['create_time'][0]).dt.total_seconds()\n",
    "    stu_kp_data = {}\n",
    "    stu_q_data = {}\n",
    "    kp_data = {}\n",
    "    q_data = {}\n",
    "    q_all_data = {}\n",
    "    kp_all_data = {}\n",
    "    stu_kp_data_an = {}\n",
    "    stu_q_data_an = {}\n",
    "    kp_data_an = {}\n",
    "    q_data_an = {}\n",
    "    last_kp_time = {}\n",
    "    last_q_time = {}\n",
    "    passed_kp_list = {}\n",
    "    passed_q_list = {}\n",
    "    processed_data= get_data(raw_data, stu_kp_data, stu_q_data, kp_data, q_data, q_all_data,kp_all_data, stu_kp_data_an, stu_q_data_an, kp_data_an, q_data_an,last_kp_time,last_q_time,passed_kp_list,passed_q_list)\n",
    "    processed_data_name = f'info_data/{data_name}_info.pkl'\n",
    "    processed_data.to_pickle(processed_data_name)\n",
    "    print(data_name, 'Side information extraction processing completed！file at：', processed_data_name)\n",
    "    processed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16377c27",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbb0ef56",
   "metadata": {},
   "source": [
    "# Get Sequence Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fb70ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this code to get sequence data in floder 'final_data/'\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "old_col = [\n",
    "    'stu_id', 'seq_id', 'kp_id', 'q_id', 'create_time', 'use_time', 'is_right',\n",
    "    'stu_kp_now_acc', 'stu_kp_now_use_time', 'stu_kp_now_len', 'stu_q_now_acc',\n",
    "    'stu_q_now_time', 'stu_q_now_len', 'q_now_acc', 'q_now_use_time',\n",
    "    'q_now_len', 'kp_now_acc', 'kp_now_use_time', 'kp_now_len', 'q_all_acc',\n",
    "    'q_all_use_time', 'q_all_len', 'kp_all_acc', 'kp_all_use_time',\n",
    "    'kp_all_len', 'stu_kp_now_acc_an', 'stu_kp_now_use_time_an',\n",
    "    'stu_kp_now_len_an', 'stu_q_now_acc_an', 'stu_q_now_time_an',\n",
    "    'stu_q_now_len_an', 'q_now_acc_an', 'q_now_use_time_an', 'q_now_len_an',\n",
    "    'kp_now_acc_an', 'kp_now_use_time_an', 'kp_now_len_an',\n",
    "    'last_kp_time_passed', 'last_q_time_passed', 'passed_kp', 'passed_q'\n",
    "]\n",
    "\n",
    "select_col = [\n",
    "    'stu_kp_now_acc',\n",
    "    'stu_kp_now_use_time',\n",
    "    'stu_kp_now_len',\n",
    "    'stu_q_now_acc',\n",
    "    'stu_q_now_time',\n",
    "    'stu_q_now_len',\n",
    "    'q_now_acc',\n",
    "    'q_now_use_time',\n",
    "    'q_now_len',\n",
    "    'kp_now_acc',\n",
    "    'kp_now_use_time',\n",
    "    'kp_now_len',\n",
    "    'last_kp_time_passed',\n",
    "    'last_q_time_passed',\n",
    "    'stu_kp_now_acc_an',\n",
    "    'stu_kp_now_use_time_an',\n",
    "    'stu_kp_now_len_an',\n",
    "    'stu_q_now_acc_an',\n",
    "    'stu_q_now_time_an',\n",
    "    'stu_q_now_len_an',\n",
    "    'q_now_acc_an',\n",
    "    'q_now_use_time_an',\n",
    "    'q_now_len_an',\n",
    "    'kp_now_acc_an',\n",
    "    'kp_now_use_time_an',\n",
    "    'kp_now_len_an',\n",
    "    'use_time'\n",
    "]\n",
    "ITEM_COL = ['stu_kp_now_acc', 'stu_kp_now_use_time', 'stu_kp_now_len', 'stu_q_now_acc',\n",
    "            'stu_q_now_time', 'stu_q_now_len', 'q_now_acc', 'q_now_use_time',\n",
    "            'q_now_len', 'kp_now_acc', 'kp_now_use_time', 'kp_now_len',\n",
    "            'last_kp_time_passed', 'last_q_time_passed']\n",
    "ITEM_AN_COL = ['stu_kp_now_acc_an', 'stu_kp_now_use_time_an', \n",
    "               'stu_kp_now_len_an', 'stu_q_now_acc_an', \n",
    "               'stu_q_now_time_an', 'stu_q_now_len_an',\n",
    "               'q_now_acc_an', 'q_now_use_time_an', \n",
    "               'q_now_len_an', 'kp_now_acc_an', \n",
    "               'kp_now_use_time_an', 'kp_now_len_an', \n",
    "               'last_kp_time_passed', 'last_q_time_passed',\n",
    "               'use_time', 'is_right']\n",
    "\n",
    "def standard_scaler(DF):\n",
    "    st_sc = StandardScaler()\n",
    "    scaler_data_x = st_sc.fit_transform(DF)\n",
    "    scaler_data_ed = pd.DataFrame(scaler_data_x, columns=DF.columns)\n",
    "    return scaler_data_ed\n",
    "def get_memory(item_list):\n",
    "    re_data = np.zeros(shape=[MEMORY_LEN, len(ITEM_AN_COL)+1])\n",
    "    len_i = len(item_list)\n",
    "    if len_i != 0:\n",
    "        get_list = item_list[-MEMORY_LEN:]\n",
    "        for i, one in enumerate(get_list):\n",
    "            one_data = list(RAW_DATA.loc[one][ITEM_AN_COL])\n",
    "            # add mask\n",
    "            one_data.append(1)\n",
    "            re_data[i,:] = one_data\n",
    "    seq_data = re_data[:, :-1]\n",
    "    padding_mask = re_data[:, -1]\n",
    "    if sum(padding_mask) == 0:\n",
    "        padding_mask[0] = 1\n",
    "    return seq_data, padding_mask\n",
    "MEMORY_LEN = 5\n",
    "all_dataset = [ 'ASSIST2009NSB', 'ASSIST2012', 'EdNet']\n",
    "for one_dataset in all_dataset:\n",
    "    data_name = one_dataset\n",
    "    print(f'{data_name} processing')\n",
    "    processed_data = pd.read_pickle(f'info_data/{data_name}_info.pkl')\n",
    "    scaler_data = processed_data[select_col]\n",
    "    scaler_data = standard_scaler(scaler_data)\n",
    "    other_data = processed_data[['is_right','passed_kp', 'passed_q']]\n",
    "    RAW_DATA = scaler_data.join(other_data)\n",
    "    del processed_data\n",
    "    del scaler_data\n",
    "    del other_data\n",
    "    all_data = list(RAW_DATA.index)\n",
    "    all_stu_kp_seq = []\n",
    "    all_stu_kp_seq_padding_mask = []\n",
    "    all_stu_q_seq = []\n",
    "    all_stu_q_seq_padding_mask = []\n",
    "    for raw_index in tqdm.tqdm(all_data, desc='Process', mininterval=0.2,ncols=80):\n",
    "        one_data = RAW_DATA.loc[raw_index]\n",
    "        stu_kp_seq, stu_kp_seq_padding_mask = get_memory(\n",
    "            list(one_data['passed_kp']))\n",
    "        stu_q_seq, stu_q_seq_padding_mask = get_memory(\n",
    "            list(one_data['passed_q']))\n",
    "        all_stu_kp_seq.append(stu_kp_seq)\n",
    "        all_stu_kp_seq_padding_mask.append(stu_kp_seq_padding_mask)\n",
    "        all_stu_q_seq.append(stu_q_seq)\n",
    "        all_stu_q_seq_padding_mask.append(stu_q_seq_padding_mask)\n",
    "    RAW_DATA['stu_kp_seq'] = all_stu_kp_seq\n",
    "    del all_stu_kp_seq\n",
    "    RAW_DATA['stu_kp_seq_padding_mask'] = all_stu_kp_seq_padding_mask\n",
    "    del all_stu_kp_seq_padding_mask\n",
    "    RAW_DATA['stu_q_seq'] = all_stu_q_seq\n",
    "    del all_stu_q_seq\n",
    "    RAW_DATA['stu_q_seq_padding_mask'] = all_stu_q_seq_padding_mask\n",
    "    del all_stu_q_seq_padding_mask\n",
    "    file_name = f'final_data/{data_name}_final_{str(MEMORY_LEN)}.pkl'\n",
    "    RAW_DATA.to_pickle(file_name)\n",
    "    print(data_name, 'sequence data extraction processing completed！file at：', file_name)\n",
    "    RAW_DATA.head()\n",
    "    del RAW_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb33bc61",
   "metadata": {},
   "source": [
    "# Partition Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cbe584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing datasets to train data and test data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "all_dataset = [ 'ASSIST2009NSB', 'ASSIST2012', 'EdNet']\n",
    "for one_dataset in all_dataset:\n",
    "    DATA_NAME = one_dataset\n",
    "    processed_data = pd.read_pickle(f'info_data/{DATA_NAME}_info.pkl')\n",
    "    all_stu = processed_data['stu_id'].value_counts().index\n",
    "    print(DATA_NAME,'number of students:',len(all_stu))\n",
    "    test_size = 3000\n",
    "    if DATA_NAME == 'EdNet':\n",
    "        test_size = 50000\n",
    "    X_test, X_train = train_test_split(all_stu, random_state=2022, test_size=test_size, shuffle=True)\n",
    "    select_stu = X_train\n",
    "    trian_data = processed_data[processed_data['stu_id'].isin(select_stu)]\n",
    "    trian_index = trian_data.index\n",
    "    test_data = processed_data[processed_data['stu_id'].isin(select_stu)==0]\n",
    "    test_index = test_data.index\n",
    "    print(DATA_NAME,'train data length:',len(trian_index))\n",
    "    print(DATA_NAME,'test data length:',len(test_index))\n",
    "    MEMORY_LEN = 5\n",
    "    RAW_DATA = pd.read_pickle(f'../DataProcess/final_data/{DATA_NAME}_final_{str(MEMORY_LEN)}.pkl')\n",
    "    one_test_data = RAW_DATA.loc[test_index]\n",
    "    one_test_data.to_pickle(f'../DataProcess/test_data/{DATA_NAME}_test_{str(MEMORY_LEN)}.pkl')\n",
    "    one_train_data = RAW_DATA.loc[trian_index]\n",
    "    one_train_data.to_pickle(f'../DataProcess/train_data/{DATA_NAME}_final_{str(MEMORY_LEN)}.pkl')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
